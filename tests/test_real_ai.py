#!/usr/bin/env python
"""
LogCortex V3 - å®é™…è¿è¡Œæµ‹è¯•
ä½¿ç”¨çœŸå®çš„æ—¥å¿—æ–‡ä»¶å’Œ AI API è¿›è¡Œæµ‹è¯•
"""
import os
import sys
import json

# æ¸…é™¤ä»£ç†è®¾ç½®ï¼Œé¿å…è¿æ¥é—®é¢˜
os.environ.pop('http_proxy', None)
os.environ.pop('https_proxy', None)
os.environ.pop('HTTP_PROXY', None)
os.environ.pop('HTTPS_PROXY', None)
os.environ.pop('all_proxy', None)
os.environ.pop('ALL_PROXY', None)

# è®¾ç½® API Key
os.environ["LOGCORTEX_API_KEY"] = "b00e23d740524abba55a3072d10bda47.Mno0AlrtfrkG18I8"

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from openai import OpenAI
from modules.analyzer import LogAnalyzer
from modules.ai_agent import (
    build_tool_specs,
    execute_tool,
    build_system_prompt,
    ContextManager,
    get_preset,
    get_quick_health_check,
    get_subsystem_summary,
)


def test_analyzer_with_real_log():
    """æµ‹è¯•åˆ†æå™¨åŠ è½½çœŸå®æ—¥å¿—"""
    log_path = "/home/qinzihang/Code/FlightLog/log_25_2025-10-17-17-20-28.ulg"

    print("=" * 60)
    print("æµ‹è¯• 1: åŠ è½½çœŸå®æ—¥å¿—æ–‡ä»¶")
    print("=" * 60)

    try:
        analyzer = LogAnalyzer(log_path)
        print(f"âœ… æ—¥å¿—åŠ è½½æˆåŠŸ")
        print(f"   ç³»ç»Ÿ: {analyzer.sys_name}")
        print(f"   å›ºä»¶: {analyzer.ver_sw}")
        print(f"   é£è¡Œæ—¶é•¿: {analyzer.duration:.1f} ç§’")
        print(f"   Topic æ•°é‡: {len(analyzer.get_available_topics())}")
        return analyzer
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        return None


def test_l1_summary_tools(analyzer):
    """æµ‹è¯• L1 æ‘˜è¦å±‚å·¥å…·"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 2: L1 æ‘˜è¦å±‚å·¥å…·")
    print("=" * 60)

    # æµ‹è¯•å¿«é€Ÿå¥åº·æ£€æŸ¥
    print("\n2.1 get_quick_health_check:")
    try:
        result = get_quick_health_check(analyzer)
        print(f"   é£è¡ŒçŠ¶æ€: {'âœ… æ­£å¸¸' if result.get('flight_ok') else 'âš ï¸ æœ‰é—®é¢˜'}")
        print(f"   é£è¡Œæ—¶é•¿: {result.get('duration_s')} ç§’")
        print(f"   æœ€å¤§é«˜åº¦: {result.get('max_alt_m')} ç±³")
        print(f"   è­¦å‘Šæ•°: {len(result.get('warnings', []))}")
        if result.get('warnings'):
            for w in result.get('warnings', [])[:3]:
                print(f"      - [{w.get('type')}] {w.get('message')}")
    except Exception as e:
        print(f"   âŒ å¤±è´¥: {e}")

    # æµ‹è¯•å­ç³»ç»Ÿæ‘˜è¦
    print("\n2.2 get_subsystem_summary:")
    for subsystem in ['gps', 'battery', 'ekf', 'imu']:
        try:
            result = get_subsystem_summary(analyzer, subsystem)
            status = result.get('status', '?')
            status_icon = {'ok': 'âœ…', 'warning': 'âš ï¸', 'error': 'âŒ'}.get(status, 'â“')
            issues = len(result.get('issues', []))
            print(f"   {subsystem}: {status_icon} {status}, é—®é¢˜: {issues}")
        except Exception as e:
            print(f"   {subsystem}: âŒ {e}")


def test_context_manager(analyzer):
    """æµ‹è¯•ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 3: ä¸Šä¸‹æ–‡ç®¡ç†å™¨")
    print("=" * 60)

    ctx = ContextManager(max_tokens=32000)
    ctx.add_message("system", build_system_prompt(analyzer))
    ctx.add_user_message("è¯·åˆ†æè¿™æ¬¡é£è¡Œ")

    print(f"   åˆå§‹ Token æ•°: {ctx.total_tokens()}")

    # æ¨¡æ‹Ÿæ·»åŠ å·¥å…·ç»“æœ
    health_check = get_quick_health_check(analyzer)
    ctx.add_tool_result("tc_1", "get_quick_health_check", health_check)

    print(f"   æ·»åŠ å·¥å…·ç»“æœå Token æ•°: {ctx.total_tokens()}")

    stats = ctx.get_stats()
    print(f"   ç»Ÿè®¡: {stats['message_count']} æ¶ˆæ¯, {stats['utilization']}% ä½¿ç”¨")


def test_glm_connection():
    """æµ‹è¯• GLM API è¿æ¥"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 4: GLM API è¿æ¥")
    print("=" * 60)

    api_key = os.environ.get("LOGCORTEX_API_KEY")
    base_url = "https://open.bigmodel.cn/api/paas/v4"

    try:
        client = OpenAI(
            api_key=api_key,
            base_url=base_url,
            timeout=60.0
        )

        # ç®€å•æµ‹è¯•
        response = client.chat.completions.create(
            model="glm-4-flash",
            messages=[{"role": "user", "content": "å›å¤ OK ä¸¤ä¸ªå­—æ¯"}],
            max_tokens=10
        )
        content = response.choices[0].message.content
        print(f"   âœ… GLM è¿æ¥æˆåŠŸ")
        print(f"   å“åº”: {content}")
        return client
    except Exception as e:
        print(f"   âŒ è¿æ¥å¤±è´¥: {e}")
        return None


def test_ai_agent_with_tools(analyzer, client):
    """æµ‹è¯•å®Œæ•´çš„ AI Agent æµç¨‹"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 5: AI Agent å®Œæ•´æµç¨‹")
    print("=" * 60)

    if not client:
        print("   â­ï¸ è·³è¿‡ (API è¿æ¥å¤±è´¥)")
        return

    tools = build_tool_specs()
    ctx = ContextManager(max_tokens=16000)
    ctx.add_message("system", build_system_prompt(analyzer))

    # ä½¿ç”¨å¿«é€Ÿæ£€æŸ¥é¢„è®¾
    preset = get_preset("quick_health")
    user_prompt = preset.user_prompt if preset else "è¯·å¿«é€Ÿæ£€æŸ¥è¿™æ¬¡é£è¡Œçš„å¥åº·çŠ¶æ€"
    ctx.add_user_message(user_prompt)

    print(f"   ç”¨æˆ·è¾“å…¥: {user_prompt[:50]}...")

    max_steps = 5
    for step in range(max_steps):
        print(f"\n   æ­¥éª¤ {step + 1}/{max_steps}:")

        try:
            resp = client.chat.completions.create(
                model="glm-4-flash",
                messages=ctx.get_messages(),
                tools=tools,
                tool_choice="auto",
                temperature=0.2,
            )
        except Exception as e:
            print(f"      âŒ API é”™è¯¯: {e}")
            break

        msg = resp.choices[0].message
        tool_calls = getattr(msg, "tool_calls", None)

        if not tool_calls:
            # å®Œæˆ
            content = msg.content or "æœªç”Ÿæˆå†…å®¹"
            print(f"      âœ… AI å“åº”å®Œæˆ")
            print(f"      Token ä½¿ç”¨: {ctx.total_tokens()}")
            print(f"\n   --- AI å“åº” ---")
            print(f"   {content[:500]}...")
            break

        # å¤„ç†å·¥å…·è°ƒç”¨
        tool_calls_data = [
            {
                "id": tc.id,
                "type": "function",
                "function": {"name": tc.function.name, "arguments": tc.function.arguments or "{}"},
            }
            for tc in tool_calls
        ]
        ctx.add_assistant_message(tool_calls=tool_calls_data)

        for tc in tool_calls:
            tool_name = tc.function.name
            print(f"      ğŸ”§ è°ƒç”¨å·¥å…·: {tool_name}")

            try:
                tool_args = json.loads(tc.function.arguments or "{}")
            except:
                tool_args = {}

            result = execute_tool(analyzer, tool_name, tool_args)
            ctx.add_tool_result(tc.id, tool_name, result)

            # æ˜¾ç¤ºç»“æœæ‘˜è¦
            if "error" not in result:
                if tool_name == "get_quick_health_check":
                    status = "æ­£å¸¸" if result.get("flight_ok") else "æœ‰é—®é¢˜"
                    print(f"         â†’ çŠ¶æ€: {status}")
                elif tool_name == "get_subsystem_summary":
                    print(f"         â†’ {result.get('subsystem')}: {result.get('status')}")
                else:
                    print(f"         â†’ å®Œæˆ")
            else:
                print(f"         â†’ é”™è¯¯: {result.get('error')}")


def main():
    print("\n" + "=" * 60)
    print("LogCortex V3 - å®é™…è¿è¡Œæµ‹è¯•")
    print("=" * 60)

    # æµ‹è¯• 1: åŠ è½½æ—¥å¿—
    analyzer = test_analyzer_with_real_log()
    if not analyzer:
        print("\nâŒ æ— æ³•åŠ è½½æ—¥å¿—ï¼Œæµ‹è¯•ç»ˆæ­¢")
        return

    # æµ‹è¯• 2: L1 å·¥å…·
    test_l1_summary_tools(analyzer)

    # æµ‹è¯• 3: ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    test_context_manager(analyzer)

    # æµ‹è¯• 4: GLM è¿æ¥
    client = test_glm_connection()

    # æµ‹è¯• 5: å®Œæ•´ AI Agent æµç¨‹
    test_ai_agent_with_tools(analyzer, client)

    print("\n" + "=" * 60)
    print("æµ‹è¯•å®Œæˆ")
    print("=" * 60)


if __name__ == "__main__":
    main()
